{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jongwoonalee/jongwoonalee.github.io/blob/main/Additinal_Project_Using_Differnt_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnDzLqf2DFld"
      },
      "source": [
        "#Panorama Creation with Advanced Feature Matching Metrics\n",
        "1. This notebook implemented a robust panorama creation pipeline using various feature matching metrics and advanced blending techniques.\n",
        "\n",
        "##  Table of Contents\n",
        "1. [Introduction and Setup](#introduction)\n",
        "2. [Image Loading and Preprocessing](#loading)\n",
        "3. [Feature Detection and Description](#features)\n",
        "4. [Advanced Feature Matching](#matching)\n",
        "5. [Homography Estimation and RANSAC](#ransac)\n",
        "6. [Image Warping and Blending](#warping)\n",
        "7. [Results and Analysis](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDqUtlyTDFle"
      },
      "source": [
        "##  1. Introduction and Setup <a name=\"introduction\"></a>\n",
        "\n",
        "### Overview\n",
        " We explored beyond traditional methods by implementing and comparing multiple distance metrics for feature matching.\n",
        "\n",
        "### Key Innovations\n",
        "- **Multi-scale feature detection** for improved coverage\n",
        "- **Advanced distance metrics** for robust matching\n",
        "- **Attention-based matching** inspired by transformer architectures\n",
        "- **Adaptive RANSAC** with automatic threshold adjustment\n",
        "- **Smooth blending** with distance transform-based feathering\n",
        "\n",
        "### Distance Metrics Explored\n",
        "1. **Chi-squared distance** - Effective for histogram comparisons\n",
        "2. **Bhattacharyya distance** - Measures similarity between probability distributions\n",
        "3. **Earth Mover's Distance (Wasserstein)** - Robust to small distribution shifts\n",
        "4. **Attention-based matching** - Uses global context for better correspondences\n",
        "5. **Traditional metrics** - Euclidean and correlation-based matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qB-4OXMDFlf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.ndimage import maximum_filter, distance_transform_edt\n",
        "from scipy.signal import convolve2d\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "# Set matplotlib parameters for better visualization\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'folder_path': '/Volumes/SSD 1T/downloads_2024_iMAC/science_library_2',\n",
        "    'max_images': 18,  # Limit number of images for processing\n",
        "    'output_filename': 'panorama_advanced_metrics.jpg',\n",
        "    'feature_params': {\n",
        "        'max_corners': 1500,\n",
        "        'grid_size': 40,\n",
        "        'window_size': 16\n",
        "    },\n",
        "    'matching_params': {\n",
        "        'threshold': 0.7,\n",
        "        'attention_threshold': 0.3\n",
        "    },\n",
        "    'ransac_params': {\n",
        "        'inlier_threshold': 4.0,\n",
        "        'iterations': 2000\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouTtH4s6DFlf"
      },
      "source": [
        "Verify Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FInx3bsUDFlf"
      },
      "outputs": [],
      "source": [
        "# Check if folder exists\n",
        "if os.path.exists(CONFIG['folder_path']):\n",
        "    print(f\"✓ Folder found: {CONFIG['folder_path']}\")\n",
        "\n",
        "    # List image files\n",
        "    image_files = [f for f in os.listdir(CONFIG['folder_path'])\n",
        "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    print(f\"✓ Found {len(image_files)} image files\")\n",
        "\n",
        "    if image_files:\n",
        "        print(\"\\nFirst 5 images:\")\n",
        "        for i, img in enumerate(image_files[:5]):\n",
        "            print(f\"  {i+1}. {img}\")\n",
        "else:\n",
        "    print(f\"✗ Folder not found: {CONFIG['folder_path']}\")\n",
        "    print(\"Please update CONFIG['folder_path'] with the correct path to your images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7vwOI7zDFlf"
      },
      "source": [
        "##  2. Image Loading and Preprocessing <a name=\"loading\"></a>\n",
        "\n",
        "### Image Loading\n",
        "The `loadImages` function handled:\n",
        "- Loading images from the specified directory\n",
        "- Converting from BGR to RGB color space\n",
        "- Normalizing pixel values to [0, 1] range\n",
        "- Creating grayscale versions for feature detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCWrKBORDFlf"
      },
      "outputs": [],
      "source": [
        "def loadImages(folder_path):\n",
        "    \"\"\"\n",
        "    Load all images from a folder and convert them to float and grayscale.\n",
        "\n",
        "    This function:\n",
        "    - Loads up to max_images (from CONFIG) from the specified folder\n",
        "    - Converts them to RGB float format (0-1 range)\n",
        "    - Creates grayscale versions for feature detection\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing images\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of RGB images, list of grayscale images)\n",
        "    \"\"\"\n",
        "    # Get sorted list of image paths (limited by CONFIG)\n",
        "    image_paths = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:CONFIG['max_images']]\n",
        "\n",
        "    images = []\n",
        "    grays = []\n",
        "\n",
        "    for i, path in enumerate(image_paths):\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Convert BGR to RGB and normalize to [0, 1] range\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "        # Convert to grayscale for feature detection\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
        "\n",
        "        images.append(img_rgb)\n",
        "        grays.append(gray)\n",
        "\n",
        "    print(f\"Loaded {len(images)} images from: {folder_path}\")\n",
        "    return images, grays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VegO_zK2DFlg"
      },
      "source": [
        "### Exposure Compensation\n",
        "Global exposure adjustment ensures consistent brightness across all images, which is crucial for seamless blending:\n",
        "- Calculates mean brightness for each image\n",
        "- Adjusts to match median brightness across all images\n",
        "- Clips gain to prevent extreme adjustments (±30% maximum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Cw-Ay4DFlg"
      },
      "outputs": [],
      "source": [
        "def compensateExposure(images):\n",
        "    \"\"\"\n",
        "    Perform global exposure adjustment across all images.\n",
        "\n",
        "    This function:\n",
        "    - Calculates the mean brightness of each image\n",
        "    - Adjusts each image to match the median brightness\n",
        "    - Clips the gain to prevent extreme adjustments\n",
        "\n",
        "    Args:\n",
        "        images (list): List of input images\n",
        "\n",
        "    Returns:\n",
        "        list: Exposure-adjusted images\n",
        "    \"\"\"\n",
        "    # Calculate mean brightness for each image\n",
        "    mean_brightness = [np.mean(img) for img in images]\n",
        "    # Target brightness is the median across all images\n",
        "    target_brightness = np.median(mean_brightness)\n",
        "\n",
        "    adjusted_images = []\n",
        "    for i, img in enumerate(images):\n",
        "        # Calculate gain to reach target brightness\n",
        "        gain = target_brightness / mean_brightness[i]\n",
        "        # Limit gain to prevent extreme adjustments (30% max change)\n",
        "        gain = np.clip(gain, 0.7, 1.3)\n",
        "        adjusted_img = img * gain\n",
        "        # Ensure pixel values stay in valid range [0, 1]\n",
        "        adjusted_images.append(np.clip(adjusted_img, 0, 1))\n",
        "\n",
        "    return adjusted_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70IaX-eyDFlg"
      },
      "source": [
        "## 3. Feature Detection and Description <a name=\"features\"></a>\n",
        "\n",
        "### Multi-Scale Harris Corner Detection\n",
        "Our enhanced feature detection implemented:\n",
        "- **Multiple scales** (1.0, 1.2, 1.5) for better feature coverage\n",
        "- **Bilateral filtering** for noise reduction while preserving edges\n",
        "- **Grid-based selection** for uniform spatial distribution\n",
        "- **Focused detection** reducing weight on bottom 35% (often less relevant areas)\n",
        "\n",
        "### Harris Corner Response\n",
        "The mathematical foundation:R = $det$(M) - $k·trace$($M$)²\n",
        "Where:\n",
        "- M is the structure tensor\n",
        "- k = 0.04 (empirically determined constant)\n",
        "- det(M) = λ₁λ₂ (product of eigenvalues)\n",
        "- trace(M) = λ₁ + λ₂ (sum of eigenvalues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmCop37QDFlg"
      },
      "outputs": [],
      "source": [
        "def getFeaturePoints(image, plot=True):\n",
        "    \"\"\"\n",
        "    Enhanced multi-scale feature detection using Harris corner detector.\n",
        "\n",
        "    Key innovations:\n",
        "    - Multiple scales for better feature coverage\n",
        "    - Bilateral filtering for noise reduction\n",
        "    - Grid-based feature selection for spatial distribution\n",
        "    - Focused detection on structural regions (reduced weight on bottom 35%)\n",
        "\n",
        "    Args:\n",
        "        image (np.array): Input image (RGB or grayscale)\n",
        "        plot (bool): Whether to visualize detected features\n",
        "\n",
        "    Returns:\n",
        "        list: List of (x, y) corner coordinates\n",
        "    \"\"\"\n",
        "    # Convert to grayscale if needed\n",
        "    if len(image.shape) == 3:\n",
        "        gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0\n",
        "    else:\n",
        "        gray = image.copy()\n",
        "\n",
        "    # Multi-scale detection for better feature coverage\n",
        "    scales = [1.0, 1.2, 1.5]\n",
        "    all_corners = []\n",
        "\n",
        "    for scale in scales:\n",
        "        # Resize image based on scale\n",
        "        if scale != 1.0:\n",
        "            scaled_gray = cv2.resize(gray, None, fx=1/scale, fy=1/scale)\n",
        "        else:\n",
        "            scaled_gray = gray\n",
        "\n",
        "        # Apply bilateral filter to reduce noise while preserving edges\n",
        "        filtered = cv2.bilateralFilter((scaled_gray * 255).astype(np.uint8), 9, 50, 50)\n",
        "        scaled_gray = filtered.astype(np.float32) / 255.0\n",
        "\n",
        "        # Compute image gradients using Sobel operator\n",
        "        Ix = cv2.Sobel(scaled_gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        Iy = cv2.Sobel(scaled_gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "\n",
        "        # Products of derivatives\n",
        "        Ix2 = Ix * Ix\n",
        "        Iy2 = Iy * Iy\n",
        "        Ixy = Ix * Iy\n",
        "\n",
        "        # Apply Gaussian window to compute weighted sums\n",
        "        window_size = 5\n",
        "        kernel = cv2.getGaussianKernel(window_size, 1.5)\n",
        "        Sx2 = cv2.filter2D(Ix2, -1, kernel @ kernel.T)\n",
        "        Sy2 = cv2.filter2D(Iy2, -1, kernel @ kernel.T)\n",
        "        Sxy = cv2.filter2D(Ixy, -1, kernel @ kernel.T)\n",
        "\n",
        "        # Harris corner response: R = det(M) - k*trace(M)^2\n",
        "        k = 0.04  # Harris detector constant\n",
        "        det = Sx2 * Sy2 - Sxy * Sxy\n",
        "        trace = Sx2 + Sy2\n",
        "        response = det - k * trace * trace\n",
        "\n",
        "        # Reduce weight on bottom portion (often contains less relevant features)\n",
        "        h, w = response.shape\n",
        "        response[int(h*0.65):, :] *= 0.2\n",
        "\n",
        "        # Threshold response to find corners\n",
        "        threshold = 0.01 * response.max()\n",
        "        corners = response > threshold\n",
        "\n",
        "        # Non-maximum suppression to avoid clustered features\n",
        "        local_maxima = maximum_filter(response, size=11)\n",
        "        corners = corners & (response == local_maxima)\n",
        "\n",
        "        # Extract corner coordinates and scale back to original size\n",
        "        y_coords, x_coords = np.where(corners)\n",
        "        for x, y in zip(x_coords, y_coords):\n",
        "            all_corners.append((x * scale, y * scale, response[y, x]))\n",
        "\n",
        "    # Sort corners by response strength\n",
        "    all_corners.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Grid-based selection for better spatial distribution\n",
        "    h, w = image.shape[:2]\n",
        "    grid_size = CONFIG['feature_params']['grid_size']\n",
        "    grid_corners = {}\n",
        "\n",
        "    for x, y, resp in all_corners:\n",
        "        grid_x = int(x / grid_size)\n",
        "        grid_y = int(y / grid_size)\n",
        "        key = (grid_x, grid_y)\n",
        "\n",
        "        # Keep only the strongest corner in each grid cell\n",
        "        if key not in grid_corners or resp > grid_corners[key][2]:\n",
        "            grid_corners[key] = (x, y, resp)\n",
        "\n",
        "    # Extract final corner points (limit to max_corners from CONFIG)\n",
        "    corner_points = [(int(x), int(y)) for x, y, _ in grid_corners.values()][:CONFIG['feature_params']['max_corners']]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
        "        if corner_points:\n",
        "            points = np.array(corner_points)\n",
        "            plt.scatter(points[:, 0], points[:, 1], c='red', s=15, marker='+')\n",
        "        plt.title(f'Detected {len(corner_points)} corners')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return corner_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1vH-ONBDFlg"
      },
      "source": [
        "### Feature Descriptors\n",
        "Two descriptor types are supported:\n",
        "1. **Gradient-based descriptors**\n",
        "   - Creates 18-bin orientation histograms\n",
        "   - Similar to SIFT descriptors\n",
        "   - Weighted by gradient magnitude\n",
        "2. **Patch-based descriptors**\n",
        "   - Normalized intensity patches\n",
        "   - Zero mean, unit variance normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA1yTGK9DFlg"
      },
      "outputs": [],
      "source": [
        "def getFeatureDescriptors(image, feature_points, mode='gradients', window_size=None):\n",
        "    \"\"\"\n",
        "    Extract robust feature descriptors for matching.\n",
        "\n",
        "    This function supports two modes:\n",
        "    1. 'gradients': Creates orientation histograms (similar to SIFT)\n",
        "    2. 'patches': Uses normalized intensity patches\n",
        "\n",
        "    Args:\n",
        "        image (np.array): Input image\n",
        "        feature_points (list): List of (x, y) feature coordinates\n",
        "        mode (str): Descriptor type ('gradients' or 'patches')\n",
        "        window_size (int): Size of the window around each feature (uses CONFIG if None)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of descriptors, list of valid feature points)\n",
        "    \"\"\"\n",
        "    if window_size is None:\n",
        "        window_size = CONFIG['feature_params']['window_size']\n",
        "\n",
        "    descriptors = []\n",
        "    valid_points = []\n",
        "    half_window = window_size // 2\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    if mode == 'gradients':\n",
        "        # Compute image gradients\n",
        "        dx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        dy = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        magnitude = np.sqrt(dx**2 + dy**2)\n",
        "        orientation = np.arctan2(dy, dx)\n",
        "\n",
        "        for x, y in feature_points:\n",
        "            # Check if window is within image bounds\n",
        "            if (y >= half_window and y < h - half_window and\n",
        "                x >= half_window and x < w - half_window):\n",
        "\n",
        "                # Extract local windows\n",
        "                mag_win = magnitude[y-half_window:y+half_window, x-half_window:x+half_window]\n",
        "                ori_win = orientation[y-half_window:y+half_window, x-half_window:x+half_window]\n",
        "\n",
        "                # Create orientation histogram (18 bins covering -π to π)\n",
        "                hist = np.zeros(18)\n",
        "                bins = np.linspace(-np.pi, np.pi, 19)\n",
        "\n",
        "                # Accumulate weighted orientations\n",
        "                for r in range(mag_win.shape[0]):\n",
        "                    for c in range(mag_win.shape[1]):\n",
        "                        bin_idx = np.digitize(ori_win[r, c], bins) - 1\n",
        "                        if 0 <= bin_idx < 18:\n",
        "                            # Weight by gradient magnitude\n",
        "                            hist[bin_idx] += mag_win[r, c]\n",
        "\n",
        "                # Normalize histogram\n",
        "                if hist.sum() > 0:\n",
        "                    hist = hist / hist.sum()\n",
        "\n",
        "                descriptors.append(hist)\n",
        "                valid_points.append((x, y))\n",
        "    else:\n",
        "        # Patch-based descriptors\n",
        "        for x, y in feature_points:\n",
        "            if (y >= half_window and y < h - half_window and\n",
        "                x >= half_window and x < w - half_window):\n",
        "\n",
        "                # Extract and flatten patch\n",
        "                window = image[y-half_window:y+half_window, x-half_window:x+half_window]\n",
        "                descriptor = window.flatten()\n",
        "\n",
        "                # Normalize descriptor (zero mean, unit variance)\n",
        "                descriptor = descriptor - descriptor.mean()\n",
        "                if descriptor.std() > 0:\n",
        "                    descriptor = descriptor / descriptor.std()\n",
        "\n",
        "                descriptors.append(descriptor)\n",
        "                valid_points.append((x, y))\n",
        "\n",
        "    return descriptors, valid_points"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  4. Advanced Feature Matching <a name=\"matching\"></a>\n",
        "\n",
        "### Distance Metrics Implementation\n",
        "\n",
        "#### 1) Chi-squared Distance\n",
        "Particularly effective for comparing probability distributions:\n",
        "$χ$² = 0.5 × $Σ$$(h₁ - h₂)²$ / $(h₁ + h₂))$\n",
        "#### 2) Bhattacharyya Distance\n",
        "Measures overlap between two distributions:\n",
        "D = -$ln$$(BC)$ where $BC$ = $Σ$$√(h₁ × h₂)$\n",
        "#### 3) Earth Mover's Distance (Wasserstein)\n",
        "Computes minimum cost to transform one distribution into another:\n",
        "- Robust to small shifts in distributions\n",
        "- Considers the \"effort\" needed to match histograms\n",
        "\n",
        "#### 4)Attention-based Matching\n",
        "Inspired by transformer architectures:\n",
        "1. Computes similarity matrix using dot products\n",
        "2. Applies softmax with temperature control\n",
        "3. Uses attention weights to find globally consistent matches\n",
        "4. Temperature parameter controls matching selectivity\n",
        "\n",
        "### 5) Ratio Test\n",
        "Implements Lowe's ratio test for robust matching:\n",
        "- Compares best match distance to second-best\n",
        "- Rejects ambiguous matches\n",
        "- Threshold typically set to 0.7-0.8"
      ],
      "metadata": {
        "id": "dEXzTzyVFCrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqWLOvWADFlh"
      },
      "outputs": [],
      "source": [
        "def chi_squared_distance(hist1, hist2):\n",
        "    \"\"\"\n",
        "    Chi-squared distance for histogram comparison.\n",
        "\n",
        "    Chi-squared distance is particularly effective for comparing\n",
        "    probability distributions like orientation histograms.\n",
        "\n",
        "    Formula: χ² = 0.5 * Σ((h1 - h2)² / (h1 + h2))\n",
        "    \"\"\"\n",
        "    return 0.5 * np.sum((hist1 - hist2)**2 / (hist1 + hist2 + 1e-10))\n",
        "\n",
        "def bhattacharyya_distance(hist1, hist2):\n",
        "    \"\"\"\n",
        "    Bhattacharyya distance for histogram similarity.\n",
        "\n",
        "    This metric measures the similarity between two probability distributions.\n",
        "    It's related to the Bhattacharyya coefficient.\n",
        "\n",
        "    Formula: D = -ln(BC) where BC = Σ√(h1 * h2)\n",
        "    \"\"\"\n",
        "    bc = np.sum(np.sqrt(hist1 * hist2))\n",
        "    return -np.log(bc + 1e-10)\n",
        "\n",
        "def earth_movers_distance(hist1, hist2):\n",
        "    \"\"\"\n",
        "    Earth Mover's Distance (Wasserstein distance).\n",
        "\n",
        "    EMD computes the minimum cost of transforming one distribution\n",
        "    into another, making it robust to small shifts.\n",
        "    \"\"\"\n",
        "    return wasserstein_distance(range(len(hist1)), range(len(hist2)), hist1, hist2)\n",
        "\n",
        "def attention_based_matching(desc1_all, desc2_all, threshold=0.3, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Attention-based matching with global context.\n",
        "\n",
        "    This method uses attention mechanisms similar to transformers\n",
        "    to find correspondences by considering global feature relationships.\n",
        "\n",
        "    Args:\n",
        "        desc1_all (np.array): Descriptors from image 1\n",
        "        desc2_all (np.array): Descriptors from image 2\n",
        "        threshold (float): Minimum similarity threshold\n",
        "        temperature (float): Softmax temperature (lower = sharper attention)\n",
        "\n",
        "    Returns:\n",
        "        list: Matched feature pairs [(idx1, idx2), ...]\n",
        "    \"\"\"\n",
        "    # Compute similarity matrix (dot product for cosine similarity)\n",
        "    similarity_matrix = desc1_all @ desc2_all.T\n",
        "\n",
        "    # Apply softmax with temperature to get attention weights\n",
        "    attention_weights = np.exp(similarity_matrix / temperature)\n",
        "    attention_weights /= attention_weights.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Find best matches using attention-weighted similarities\n",
        "    matches = []\n",
        "    used_j = set()\n",
        "\n",
        "    for i in range(len(desc1_all)):\n",
        "        # Weight similarities by attention\n",
        "        weighted_similarities = attention_weights[i] * similarity_matrix[i]\n",
        "        # Sort by weighted similarity\n",
        "        sorted_indices = np.argsort(weighted_similarities)[::-1]\n",
        "\n",
        "        # Find best unused match\n",
        "        for j in sorted_indices:\n",
        "            if j not in used_j and weighted_similarities[j] > threshold:\n",
        "                matches.append((i, j))\n",
        "                used_j.add(j)\n",
        "                break\n",
        "\n",
        "    return matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x39_oYXrDFlh"
      },
      "source": [
        "Main Matching Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU_iZuZrDFlh"
      },
      "outputs": [],
      "source": [
        "def match2Images(points1, descriptors1, points2, descriptors2,\n",
        "                 threshold=0.7, distance_type='chi_squared', plot=True):\n",
        "    \"\"\"\n",
        "    Enhanced matching with multiple distance metrics.\n",
        "\n",
        "    This function supports various distance metrics for finding\n",
        "    correspondences between feature descriptors from two images.\n",
        "\n",
        "    Supported metrics:\n",
        "    - 'euclidean': Standard L2 distance\n",
        "    - 'correlation': Correlation coefficient\n",
        "    - 'chi_squared': Chi-squared distance for histograms\n",
        "    - 'bhattacharyya': Bhattacharyya distance\n",
        "    - 'earth_movers': Earth Mover's Distance\n",
        "    - 'attention': Attention-based matching\n",
        "\n",
        "    Args:\n",
        "        points1, points2: Feature point coordinates\n",
        "        descriptors1, descriptors2: Feature descriptors\n",
        "        threshold: Distance threshold for matches\n",
        "        distance_type: Type of distance metric to use\n",
        "        plot: Whether to visualize matches\n",
        "\n",
        "    Returns:\n",
        "        list: Matched pairs [(idx1, idx2), ...]\n",
        "    \"\"\"\n",
        "    if not descriptors1 or not descriptors2:\n",
        "        return []\n",
        "\n",
        "    desc1 = np.array(descriptors1)\n",
        "    desc2 = np.array(descriptors2)\n",
        "\n",
        "    # Special handling for attention-based matching\n",
        "    if distance_type == 'attention':\n",
        "        matches = attention_based_matching(desc1, desc2, threshold)\n",
        "        print(f\"Found {len(matches)} matches using attention\")\n",
        "        return matches\n",
        "\n",
        "    # Compute distance matrix for other metrics\n",
        "    distances = np.zeros((len(desc1), len(desc2)))\n",
        "\n",
        "    if distance_type == 'euclidean':\n",
        "        distances = cdist(desc1, desc2, 'euclidean')\n",
        "    elif distance_type == 'correlation':\n",
        "        for i in range(len(desc1)):\n",
        "            for j in range(len(desc2)):\n",
        "                corr = np.corrcoef(desc1[i], desc2[j])[0, 1]\n",
        "                distances[i, j] = 1 - abs(corr)\n",
        "    elif distance_type == 'chi_squared':\n",
        "        for i in range(len(desc1)):\n",
        "            for j in range(len(desc2)):\n",
        "                distances[i, j] = chi_squared_distance(desc1[i], desc2[j])\n",
        "    elif distance_type == 'bhattacharyya':\n",
        "        for i in range(len(desc1)):\n",
        "            for j in range(len(desc2)):\n",
        "                distances[i, j] = bhattacharyya_distance(desc1[i], desc2[j])\n",
        "    elif distance_type == 'earth_movers':\n",
        "        for i in range(len(desc1)):\n",
        "            for j in range(len(desc2)):\n",
        "                distances[i, j] = earth_movers_distance(desc1[i], desc2[j])\n",
        "\n",
        "    # Apply ratio test (Lowe's ratio test)\n",
        "    matches = []\n",
        "    for i in range(len(desc1)):\n",
        "        sorted_idx = np.argsort(distances[i])\n",
        "        best = sorted_idx[0]\n",
        "        second_best = sorted_idx[1]\n",
        "\n",
        "        # Check if best match is significantly better than second best\n",
        "        if distances[i, best] < threshold * distances[i, second_best]:\n",
        "            matches.append((i, best))\n",
        "\n",
        "    if plot and matches:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(121)\n",
        "        pts1 = np.array([points1[m[0]] for m in matches[:50]])\n",
        "        plt.scatter(pts1[:, 0], pts1[:, 1], c='red', s=10)\n",
        "        plt.title(f'Image 1 - {len(matches)} matches ({distance_type})')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        pts2 = np.array([points2[m[1]] for m in matches[:50]])\n",
        "        plt.scatter(pts2[:, 0], pts2[:, 1], c='blue', s=10)\n",
        "        plt.title('Image 2')\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"Found {len(matches)} matches using {distance_type}\")\n",
        "    return matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ9gtwrcDFlh"
      },
      "source": [
        "## 5. Homography Estimation and RANSAC <a name=\"ransac\"></a>\n",
        "\n",
        "### RANSAC Algorithm\n",
        "Random Sample Consensus for robust homography estimation:\n",
        "\n",
        "1. **Random Sampling**: Select 4 point correspondences\n",
        "2. **Homography Calculation**: Compute H using Direct Linear Transform (DLT)\n",
        "3. **Inlier Detection**: Count points with reprojection error < threshold\n",
        "4. **Iteration**: Repeat and keep best homography\n",
        "\n",
        "### Adaptive Threshold\n",
        "Our implementation features adaptive inlier threshold:\n",
        "- Analyzes motion magnitude statistics\n",
        "- Sets threshold to 75th percentile of motion magnitudes\n",
        "- Prevents overly strict or loose criteria\n",
        "\n",
        "### Direct Linear Transform (DLT)\n",
        "For each correspondence (x,y) → (u,v):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXm9AOGaDFlh"
      },
      "outputs": [],
      "source": [
        "def refineMatches(points1, points2, matches, inlier_threshold=4.0, iterations=2000, plot=True):\n",
        "    \"\"\"\n",
        "    RANSAC-based homography estimation with adaptive threshold.\n",
        "\n",
        "    RANSAC process:\n",
        "    1. Randomly select 4 matches (minimum for homography)\n",
        "    2. Compute homography from these 4 points\n",
        "    3. Count inliers based on reprojection error\n",
        "    4. Repeat and keep best homography\n",
        "\n",
        "    Args:\n",
        "        points1, points2: Feature coordinates in both images\n",
        "        matches: Initial matches [(idx1, idx2), ...]\n",
        "        inlier_threshold: Maximum error for inliers (pixels)\n",
        "        iterations: Number of RANSAC iterations\n",
        "        plot: Whether to visualize inlier distribution\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_homography, inlier_indices)\n",
        "    \"\"\"\n",
        "    if len(matches) < 4:\n",
        "        return None, []\n",
        "\n",
        "    # Convert matches to point correspondences\n",
        "    pts1 = np.float32([points1[m[0]] for m in matches])\n",
        "    pts2 = np.float32([points2[m[1]] for m in matches])\n",
        "\n",
        "    # Adaptive threshold based on motion statistics\n",
        "    motion_magnitudes = np.linalg.norm(pts2 - pts1, axis=1)\n",
        "    adaptive_threshold = min(inlier_threshold, np.percentile(motion_magnitudes, 75))\n",
        "\n",
        "    best_H = None\n",
        "    best_inliers = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Randomly select 4 matches\n",
        "        idx = np.random.choice(len(matches), 4, replace=False)\n",
        "        src_pts = pts1[idx]\n",
        "        dst_pts = pts2[idx]\n",
        "\n",
        "        # Build linear system for homography estimation\n",
        "        # Using Direct Linear Transform (DLT)\n",
        "        A = []\n",
        "        for i in range(4):\n",
        "            x, y = src_pts[i]\n",
        "            u, v = dst_pts[i]\n",
        "            # Each correspondence gives 2 equations\n",
        "            A.append([-x, -y, -1, 0, 0, 0, u*x, u*y, u])\n",
        "            A.append([0, 0, 0, -x, -y, -1, v*x, v*y, v])\n",
        "\n",
        "        A = np.array(A)\n",
        "        try:\n",
        "            # Solve using SVD\n",
        "            _, _, V = np.linalg.svd(A)\n",
        "            H = V[-1].reshape(3, 3)\n",
        "            H = H / H[2, 2]  # Normalize so H[2,2] = 1\n",
        "\n",
        "            # Count inliers\n",
        "            inliers = []\n",
        "            for i in range(len(pts1)):\n",
        "                # Apply homography\n",
        "                pt = np.append(pts1[i], 1)\n",
        "                transformed = H @ pt\n",
        "                if transformed[2] != 0:\n",
        "                    transformed = transformed[:2] / transformed[2]\n",
        "                    # Calculate reprojection error\n",
        "                    error = np.linalg.norm(transformed - pts2[i])\n",
        "                    if error < adaptive_threshold:\n",
        "                        inliers.append(i)\n",
        "\n",
        "            # Update best if this has more inliers\n",
        "            if len(inliers) > len(best_inliers):\n",
        "                best_H = H\n",
        "                best_inliers = inliers\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Print statistics\n",
        "    if best_inliers:\n",
        "        residuals = []\n",
        "        for i in best_inliers:\n",
        "            pt = np.append(pts1[i], 1)\n",
        "            transformed = best_H @ pt\n",
        "            transformed = transformed[:2] / transformed[2]\n",
        "            residuals.append(np.linalg.norm(transformed - pts2[i]))\n",
        "        avg_residual = np.mean(residuals)\n",
        "        print(f\"RANSAC: {len(best_inliers)} inliers, avg residual: {avg_residual:.2f}\")\n",
        "\n",
        "    if plot and best_inliers:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        inlier_mask = np.zeros(len(matches), dtype=bool)\n",
        "        for i in best_inliers:\n",
        "            inlier_mask[i] = True\n",
        "        plt.bar(range(len(matches)), inlier_mask.astype(int))\n",
        "        plt.title(f'{len(best_inliers)} inliers after RANSAC')\n",
        "        plt.ylabel('Inlier')\n",
        "        plt.xlabel('Match index')\n",
        "        plt.show()\n",
        "\n",
        "    return best_H, best_inliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sLtkgXBDFli"
      },
      "source": [
        "## 6. Image Warping and Blending <a name=\"warping\"></a>\n",
        "\n",
        "### Warping Pipeline\n",
        "1. **Exposure compensation** for consistent brightness\n",
        "2. **Cumulative transformation** calculation from center image\n",
        "3. **Canvas size** determination from transformed corners\n",
        "4. **Progressive warping** starting from center\n",
        "5. **Smooth blending** of overlapping regions\n",
        "\n",
        "### Advanced Blending Technique\n",
        "Our blending method uses distance transforms:\n",
        "\n",
        "1. **Distance Maps**: Calculate distance from mask edges\n",
        "2. **Weight Calculation**: Proportional to distance from edge\n",
        "3. **Overlap Handling**: Smooth transitions in overlap regions\n",
        "4. **Gaussian Smoothing**: Final smoothing of blend weights\n",
        "5. **Per-channel Blending**: Separate blending for R, G, B channels\n",
        "\n",
        "### Mathematical Formulation\n",
        "For overlapping regions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnDw_19EDFli"
      },
      "outputs": [],
      "source": [
        "def smoothBlend(img1, img2, mask1, mask2):\n",
        "    \"\"\"\n",
        "    Smooth blending with feathering for seamless transitions.\n",
        "\n",
        "    This function uses distance transforms to create smooth\n",
        "    blend weights in overlapping regions.\n",
        "\n",
        "    Args:\n",
        "        img1, img2: Images to blend\n",
        "        mask1, mask2: Binary masks indicating valid pixels\n",
        "\n",
        "    Returns:\n",
        "        np.array: Blended image\n",
        "    \"\"\"\n",
        "    # Create distance maps from mask edges\n",
        "    dist1 = cv2.distanceTransform((mask1 > 0).astype(np.uint8), cv2.DIST_L2, 5)\n",
        "    dist2 = cv2.distanceTransform((mask2 > 0).astype(np.uint8), cv2.DIST_L2, 5)\n",
        "\n",
        "    # Normalize distances\n",
        "    dist1 = dist1 / (dist1.max() + 1e-5)\n",
        "    dist2 = dist2 / (dist2.max() + 1e-5)\n",
        "\n",
        "    # Create blend weights\n",
        "    overlap = mask1 * mask2\n",
        "    blend_weight = np.zeros_like(mask1)\n",
        "\n",
        "    # Smooth transition in overlap regions\n",
        "    overlap_indices = np.where(overlap > 0)\n",
        "    if len(overlap_indices[0]) > 0:\n",
        "        weights1 = dist1[overlap_indices]\n",
        "        weights2 = dist2[overlap_indices]\n",
        "        # Weight proportional to distance from edge\n",
        "        blend_weight[overlap_indices] = weights1 / (weights1 + weights2 + 1e-6)\n",
        "\n",
        "    # Non-overlap regions get full weight\n",
        "    blend_weight[mask1 > mask2] = 1\n",
        "    blend_weight[mask2 > mask1] = 0\n",
        "\n",
        "    # Apply Gaussian blur for smoother transitions\n",
        "    blend_weight = cv2.GaussianBlur(blend_weight, (31, 31), 10)\n",
        "\n",
        "    # Blend images channel by channel\n",
        "    result = np.zeros_like(img1)\n",
        "    for c in range(3):\n",
        "        result[:, :, c] = img1[:, :, c] * blend_weight + img2[:, :, c] * (1 - blend_weight)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdEJAb_IDFli"
      },
      "source": [
        "Complete Warping Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v9Ny5kZDFli"
      },
      "outputs": [],
      "source": [
        "def warpImages(images, homographies, plot=True):\n",
        "    \"\"\"\n",
        "    Warp all images to create the final panorama.\n",
        "\n",
        "    This function:\n",
        "    1. Applies exposure compensation\n",
        "    2. Computes cumulative transformations from center\n",
        "    3. Calculates output canvas size\n",
        "    4. Warps and blends images progressively\n",
        "\n",
        "    Args:\n",
        "        images: List of input images\n",
        "        homographies: List of pairwise homographies\n",
        "        plot: Whether to display the result\n",
        "\n",
        "    Returns:\n",
        "        np.array: Final panorama\n",
        "    \"\"\"\n",
        "    n = len(images)\n",
        "\n",
        "    print(f\"Creating panorama with all {n} images...\")\n",
        "\n",
        "    # Pre-process images for consistent exposure\n",
        "    adjusted_images = compensateExposure(images)\n",
        "\n",
        "    # Calculate cumulative transformations from center\n",
        "    center_idx = n // 2\n",
        "    H_cumulative = [None] * n\n",
        "    H_cumulative[center_idx] = np.eye(3)\n",
        "\n",
        "    # Build transformations going left from center\n",
        "    current_H = np.eye(3)\n",
        "    for i in range(center_idx - 1, -1, -1):\n",
        "        if i < len(homographies) and homographies[i] is not None:\n",
        "            # Chain transformations\n",
        "            current_H = current_H @ np.linalg.inv(homographies[i])\n",
        "            H_cumulative[i] = current_H\n",
        "        else:\n",
        "            H_cumulative[i] = None\n",
        "\n",
        "    # Build transformations going right from center\n",
        "    current_H = np.eye(3)\n",
        "    for i in range(center_idx, n - 1):\n",
        "        if i < len(homographies) and homographies[i] is not None:\n",
        "            # Chain transformations\n",
        "            current_H = current_H @ homographies[i]\n",
        "            H_cumulative[i + 1] = current_H\n",
        "        else:\n",
        "            H_cumulative[i + 1] = None\n",
        "\n",
        "    # Find valid indices (images with valid homographies)\n",
        "    valid_indices = [i for i in range(n) if H_cumulative[i] is not None]\n",
        "\n",
        "    if len(valid_indices) < 2:\n",
        "        return images[center_idx]\n",
        "\n",
        "    # Calculate output canvas size\n",
        "    all_corners = []\n",
        "    for idx in valid_indices:\n",
        "        h, w = adjusted_images[idx].shape[:2]\n",
        "        # Define corner points\n",
        "        corners = np.array([[0, 0, 1], [w, 0, 1], [w, h, 1], [0, h, 1]]).T\n",
        "\n",
        "        # Transform corners\n",
        "        warped_corners = H_cumulative[idx] @ corners\n",
        "        warped_corners = warped_corners[:2] / warped_corners[2]\n",
        "        all_corners.append(warped_corners.T)\n",
        "\n",
        "    # Find bounding box\n",
        "    all_corners = np.vstack(all_corners)\n",
        "    min_xy = np.floor(all_corners.min(axis=0)).astype(int)\n",
        "    max_xy = np.ceil(all_corners.max(axis=0)).astype(int)\n",
        "\n",
        "    out_width = max_xy[0] - min_xy[0]\n",
        "    out_height = max_xy[1] - min_xy[1]\n",
        "\n",
        "    # Limit output size to prevent memory issues\n",
        "    max_width = 10000\n",
        "    max_height = 5000\n",
        "    if out_width > max_width or out_height > max_height:\n",
        "        scale = min(max_width/out_width, max_height/out_height)\n",
        "        out_width = int(out_width * scale)\n",
        "        out_height = int(out_height * scale)\n",
        "\n",
        "    # Translation to ensure all pixels are positive\n",
        "    translation = np.array([[1, 0, -min_xy[0]], [0, 1, -min_xy[1]], [0, 0, 1]])\n",
        "\n",
        "    # Start with center image\n",
        "    center_H = translation @ H_cumulative[center_idx]\n",
        "    result = cv2.warpPerspective(\n",
        "        (adjusted_images[center_idx] * 255).astype(np.uint8),\n",
        "        center_H.astype(np.float32),\n",
        "        (out_width, out_height)\n",
        "    )\n",
        "    result = result.astype(np.float32) / 255.0\n",
        "\n",
        "    # Add remaining images, starting from closest to center\n",
        "    indices_by_distance = sorted(valid_indices, key=lambda x: abs(x - center_idx))\n",
        "\n",
        "    for idx in indices_by_distance:\n",
        "        if idx == center_idx:\n",
        "            continue\n",
        "\n",
        "        # Warp current image\n",
        "        H_total = translation @ H_cumulative[idx]\n",
        "\n",
        "        warped = cv2.warpPerspective(\n",
        "            (adjusted_images[idx] * 255).astype(np.uint8),\n",
        "            H_total.astype(np.float32),\n",
        "            (out_width, out_height)\n",
        "        )\n",
        "        warped = warped.astype(np.float32) / 255.0\n",
        "\n",
        "        # Create masks for blending\n",
        "        mask_result = (result.sum(axis=2) > 0).astype(float)\n",
        "        mask_warped = (warped.sum(axis=2) > 0).astype(float)\n",
        "\n",
        "        # Smooth blending\n",
        "        result = smoothBlend(result, warped, mask_result, mask_warped)\n",
        "\n",
        "        print(f\"Added image {idx + 1}\")\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.imshow(result)\n",
        "        plt.title(f'Final Panorama (All {len(valid_indices)} images)')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw6RrU9_DFli"
      },
      "source": [
        "7. Main Execution and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjFJD-tADFli"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution pipeline for panorama creation.\n",
        "\n",
        "    Steps:\n",
        "    1. Load all images\n",
        "    2. Detect features in all images\n",
        "    3. Match features using multiple metrics\n",
        "    4. Estimate homographies with RANSAC\n",
        "    5. Create final panorama\n",
        "    \"\"\"\n",
        "    folder = CONFIG['folder_path']\n",
        "\n",
        "    # Step 1: Load all images\n",
        "    print(\"Step 1: Loading all images...\")\n",
        "    images, grays = loadImages(folder)\n",
        "\n",
        "    # Step 2: Feature detection\n",
        "    print(\"\\nStep 2: Detecting features in all images...\")\n",
        "    all_points = []\n",
        "    all_descriptors = []\n",
        "\n",
        "    for i, gray in enumerate(grays):\n",
        "        print(f\"Processing image {i+1}/{len(grays)}\")\n",
        "        points = getFeaturePoints(gray, plot=(i==0))\n",
        "        descriptors, valid_points = getFeatureDescriptors(gray, points, mode='gradients')\n",
        "        all_points.append(valid_points)\n",
        "        all_descriptors.append(descriptors)\n",
        "        print(f\"Image {i+1}: {len(valid_points)} features\")\n",
        "\n",
        "    # Step 3: Feature matching with different metrics\n",
        "    print(\"\\nStep 3: Matching features with advanced metrics...\")\n",
        "    homographies = []\n",
        "\n",
        "    # Distance metrics in order of preference\n",
        "    distance_metrics = ['chi_squared', 'bhattacharyya', 'attention', 'euclidean']\n",
        "\n",
        "    for i in range(len(images)-1):\n",
        "        print(f\"\\nMatching images {i+1} and {i+2}:\")\n",
        "\n",
        "        best_matches = []\n",
        "        best_metric = None\n",
        "\n",
        "        # Try different distance metrics\n",
        "        for metric in distance_metrics:\n",
        "            try:\n",
        "                threshold = CONFIG['matching_params']['attention_threshold'] if metric == 'attention' else CONFIG['matching_params']['threshold']\n",
        "                matches = match2Images(\n",
        "                    all_points[i], all_descriptors[i],\n",
        "                    all_points[i+1], all_descriptors[i+1],\n",
        "                    threshold=threshold,\n",
        "                    distance_type=metric,\n",
        "                    plot=False\n",
        "                )\n",
        "\n",
        "                if len(matches) > len(best_matches):\n",
        "                    best_matches = matches\n",
        "                    best_metric = metric\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed with {metric}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Best metric: {best_metric} with {len(best_matches)} matches\")\n",
        "\n",
        "        # RANSAC refinement\n",
        "        if len(best_matches) >= 10:\n",
        "            H, inliers = refineMatches(\n",
        "                all_points[i], all_points[i+1], best_matches,\n",
        "                inlier_threshold=CONFIG['ransac_params']['inlier_threshold'],\n",
        "                iterations=CONFIG['ransac_params']['iterations'],\n",
        "                plot=(i==0)\n",
        "            )\n",
        "            if H is not None and len(inliers) >= 20:\n",
        "                homographies.append(H)\n",
        "                print(f\"Homography estimated with {len(inliers)} inliers\")\n",
        "            else:\n",
        "                homographies.append(None)\n",
        "                print(\"Failed to estimate reliable homography\")\n",
        "        else:\n",
        "            homographies.append(None)\n",
        "            print(\"Too few matches\")\n",
        "\n",
        "    # Step 4: Create panorama\n",
        "    print(\"\\nStep 4: Creating panorama...\")\n",
        "    panorama = warpImages(images, homographies)\n",
        "\n",
        "    # Save result\n",
        "    output_path = os.path.join(folder, CONFIG['output_filename'])\n",
        "    cv2.imwrite(output_path, (panorama * 255).astype(np.uint8)[:, :, ::-1])\n",
        "    print(f\"\\nPanorama saved to: {output_path}\")\n",
        "\n",
        "    return panorama\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    panorama = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axmv2xe3DFli"
      },
      "source": [
        "## 🎉 Conclusion\n",
        "\n",
        "This panorama creation pipeline demonstrates the effectiveness of using multiple distance metrics for feature matching. The combination of:\n",
        "- Advanced feature detection\n",
        "- Multiple matching strategies\n",
        "- Robust homography estimation\n",
        "- Sophisticated blending techniques\n",
        "\n",
        "produces high-quality panoramas even with challenging image sets. The modular design allows easy experimentation with different components and parameters."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}