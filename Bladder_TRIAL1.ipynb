{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqaYXh25DkmXRf+Yh+wKS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jongwoonalee/jongwoonalee.github.io/blob/main/Bladder_TRIAL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nC1kZhsDytl"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "FlexAttention-based Multi-Instance Learning for Bladder Cancer Classification\n",
        "FINAL CORRECT VERSION:\n",
        "- 1024x1024 megapatch → 16개 256x256 patches\n",
        "- 각 patch별로: LR(64x64) + HR(256x256) + Global(64x64)\n",
        "- FlexAttention으로 중요한 HR patches만 선택\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from skimage.filters import threshold_otsu\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import hashlib\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# RTX 6000 Ada x2 설정\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Found {num_gpus} GPUs\")\n",
        "    for i in range(num_gpus):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 기존 데이터 로딩 함수들 (그대로 유지)\n",
        "# =============================================================================\n",
        "\n",
        "def extract_identifier(filename):\n",
        "    \"\"\"Extract patient ID from filename\"\"\"\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    if '[' in name:\n",
        "        name = name.split('[')[0].strip()\n",
        "\n",
        "    m1 = re.match(r'^S(\\d+)-(\\d+)(?:_\\d{4}-\\d{2}-\\d{2})?', name)\n",
        "    if m1:\n",
        "        slide = m1.group(1)\n",
        "        patch = m1.group(2)\n",
        "        if len(patch) == 3:\n",
        "            patch_padded = \"000\" + patch\n",
        "        elif len(patch) == 4:\n",
        "            patch_padded = \"00\" + patch\n",
        "        elif len(patch) == 5:\n",
        "            patch_padded = \"0\" + patch\n",
        "        else:\n",
        "            patch_padded = patch\n",
        "        return f\"S{slide}{patch_padded}\"\n",
        "\n",
        "    m2 = re.match(r'^S(\\d+)[,;]', name)\n",
        "    if m2:\n",
        "        slide_id = m2.group(1)\n",
        "        return f\"S{slide_id}\", ext\n",
        "\n",
        "    m3 = re.match(r'^S(\\d{8}|\\d{7}|\\d{6})', name)\n",
        "    if m3:\n",
        "        slide_id = m3.group(1)\n",
        "        return f\"S{slide_id}\", ext\n",
        "\n",
        "    return None, ext\n",
        "\n",
        "def convert_file_id_to_excel_format(file_id):\n",
        "    \"\"\"Convert file ID to Excel format\"\"\"\n",
        "    if file_id is None:\n",
        "        return None\n",
        "\n",
        "    file_id = str(file_id).strip()\n",
        "    if \"-\" in file_id:\n",
        "        parts = file_id.split(\"-\")\n",
        "        if len(parts) == 2 and parts[1].isdigit():\n",
        "            patch = parts[1]\n",
        "            if len(patch) == 3:\n",
        "                padded_number = \"000\" + patch\n",
        "            elif len(patch) == 4:\n",
        "                padded_number = \"00\" + patch\n",
        "            elif len(patch) == 5:\n",
        "                padded_number = \"0\" + patch\n",
        "            else:\n",
        "                padded_number = patch\n",
        "            return f\"{parts[0]}{padded_number}\"\n",
        "    elif len(file_id) > 3 and file_id.startswith(\"S\"):\n",
        "        return file_id\n",
        "\n",
        "    return None\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 핵심! 메가패치 처리 함수 (완전히 새로운 방식)\n",
        "# =============================================================================\n",
        "\n",
        "def split_megapatch_to_patches(megapatch_path, grid_size=4):\n",
        "    \"\"\"\n",
        "    STEP 1: 1024x1024 메가패치를 4x4=16개의 256x256 패치로 분할\n",
        "\n",
        "    Args:\n",
        "        megapatch_path: 1024x1024 메가패치 경로\n",
        "        grid_size: 그리드 크기 (4x4 = 16개 패치)\n",
        "\n",
        "    Returns:\n",
        "        list: 16개의 256x256 패치들\n",
        "        list: 각 패치의 위치 정보 (i, j)\n",
        "    \"\"\"\n",
        "    # 1024x1024 메가패치 읽기\n",
        "    megapatch = cv2.imread(megapatch_path)\n",
        "    if megapatch is None:\n",
        "        raise ValueError(f\"Cannot read megapatch: {megapatch_path}\")\n",
        "\n",
        "    megapatch = cv2.cvtColor(megapatch, cv2.COLOR_BGR2RGB)\n",
        "    h, w = megapatch.shape[:2]\n",
        "\n",
        "    # 각 패치 크기 계산: 1024/4 = 256\n",
        "    patch_size = h // grid_size  # 256x256\n",
        "\n",
        "    patches = []\n",
        "    positions = []\n",
        "\n",
        "    # 4x4 그리드로 분할\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            y_start = i * patch_size\n",
        "            x_start = j * patch_size\n",
        "            y_end = y_start + patch_size\n",
        "            x_end = x_start + patch_size\n",
        "\n",
        "            # 256x256 패치 추출\n",
        "            patch = megapatch[y_start:y_end, x_start:x_end]\n",
        "            patches.append(patch)\n",
        "            positions.append((i, j))\n",
        "\n",
        "    return patches, positions\n",
        "\n",
        "def create_three_streams_from_patch(patch_256, megapatch_1024):\n",
        "    \"\"\"\n",
        "    STEP 2: 각 256x256 패치로부터 3-stream 생성\n",
        "\n",
        "    Args:\n",
        "        patch_256: 256x256 패치 (numpy array)\n",
        "        megapatch_1024: 전체 1024x1024 메가패치 (Global용)\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'lr': 64x64 LR 패치,\n",
        "            'hr': 256x256 HR 패치 (원본),\n",
        "            'global': 64x64 Global 컨텍스트\n",
        "        }\n",
        "    \"\"\"\n",
        "    # 1. LR: 256x256 → 64x64 다운샘플링\n",
        "    lr_patch = cv2.resize(patch_256, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # 2. HR: 256x256 원본 그대로\n",
        "    hr_patch = patch_256.copy()\n",
        "\n",
        "    # 3. Global: 전체 1024x1024 → 64x64 (매우 작은 overview)\n",
        "    global_context = cv2.resize(megapatch_1024, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    return {\n",
        "        'lr': lr_patch,         # 64x64 LR\n",
        "        'hr': hr_patch,         # 256x256 HR\n",
        "        'global': global_context # 64x64 Global\n",
        "    }\n",
        "\n",
        "def process_megapatch_complete(megapatch_path):\n",
        "    \"\"\"\n",
        "    STEP 3: 메가패치 전체 처리 - 1024x1024 → 16개 패치 → 각각 3-stream\n",
        "\n",
        "    Args:\n",
        "        megapatch_path: 1024x1024 메가패치 경로\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'lr_patches': 16개의 64x64 LR 패치들,\n",
        "            'hr_patches': 16개의 256x256 HR 패치들,\n",
        "            'global_tokens': 16개의 64x64 Global 토큰들 (모두 동일),\n",
        "            'positions': 패치 위치 정보\n",
        "        }\n",
        "    \"\"\"\n",
        "    # 원본 메가패치 읽기\n",
        "    megapatch = cv2.imread(megapatch_path)\n",
        "    if megapatch is None:\n",
        "        raise ValueError(f\"Cannot read megapatch: {megapatch_path}\")\n",
        "    megapatch = cv2.cvtColor(megapatch, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # STEP 1: 1024x1024 → 16개 256x256 패치로 분할\n",
        "    patches_256, positions = split_megapatch_to_patches(megapatch_path)\n",
        "\n",
        "    # STEP 2: 각 패치별로 3-stream 생성\n",
        "    lr_patches = []\n",
        "    hr_patches = []\n",
        "    global_tokens = []\n",
        "\n",
        "    for patch_256 in patches_256:\n",
        "        streams = create_three_streams_from_patch(patch_256, megapatch)\n",
        "\n",
        "        lr_patches.append(streams['lr'])        # 64x64\n",
        "        hr_patches.append(streams['hr'])        # 256x256\n",
        "        global_tokens.append(streams['global']) # 64x64 (전부 동일한 global context)\n",
        "\n",
        "    return {\n",
        "        'lr_patches': lr_patches,     # 16개 × 64x64\n",
        "        'hr_patches': hr_patches,     # 16개 × 256x256\n",
        "        'global_tokens': global_tokens, # 16개 × 64x64 (모두 동일)\n",
        "        'positions': positions        # 16개 위치 정보\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 3. ResNet 기반 Feature Extractor (64x64와 256x256용)\n",
        "# =============================================================================\n",
        "\n",
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet18 기반 feature extractor\n",
        "    - 64x64 이미지용 (LR, Global)\n",
        "    - 256x256 이미지용 (HR)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=384, pretrained=True):\n",
        "        super(ResNetFeatureExtractor, self).__init__()\n",
        "\n",
        "        # ResNet18 backbone\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # avgpool, fc 제거\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Feature projection\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(512, feature_dim),\n",
        "            nn.LayerNorm(feature_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, 3, H, W] - 64x64 또는 256x256\n",
        "        Returns:\n",
        "            [batch_size, feature_dim] - feature vectors\n",
        "        \"\"\"\n",
        "        # Feature extraction\n",
        "        features = self.backbone(x)      # [B, 512, H', W']\n",
        "        pooled = self.avgpool(features)  # [B, 512, 1, 1]\n",
        "        flattened = pooled.view(pooled.size(0), -1)  # [B, 512]\n",
        "        projected = self.projection(flattened)       # [B, feature_dim]\n",
        "\n",
        "        return projected\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 핵심! High-Resolution Feature Selection Module\n",
        "# =============================================================================\n",
        "\n",
        "class HRFeatureSelector(nn.Module):\n",
        "    \"\"\"\n",
        "    FlexAttention의 핵심: LR attention에 기반해서 중요한 HR features만 선택\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold=0.1, max_selection_ratio=0.5):\n",
        "        super(HRFeatureSelector, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.max_selection_ratio = max_selection_ratio\n",
        "\n",
        "    def forward(self, lr_attention_scores, hr_features):\n",
        "        \"\"\"\n",
        "        LR의 attention scores에 기반해서 중요한 HR features 선택\n",
        "\n",
        "        Args:\n",
        "            lr_attention_scores: [batch_size, 16] - LR 패치들의 attention scores\n",
        "            hr_features: [batch_size, 16, feature_dim] - HR 패치들의 features\n",
        "\n",
        "        Returns:\n",
        "            selected_hr_features: [batch_size, num_selected, feature_dim] - 선택된 HR features\n",
        "            selection_mask: [batch_size, 16] - 선택 마스크 (시각화용)\n",
        "        \"\"\"\n",
        "        batch_size, num_patches, feature_dim = hr_features.shape\n",
        "        max_selections = int(num_patches * self.max_selection_ratio)  # 최대 8개 선택\n",
        "\n",
        "        selected_hr_features = []\n",
        "        selection_masks = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # 이 샘플의 attention scores\n",
        "            att_scores = lr_attention_scores[b]  # [16]\n",
        "\n",
        "            # Dynamic threshold 계산 (Otsu 또는 percentile)\n",
        "            try:\n",
        "                threshold_val = threshold_otsu(att_scores.detach().cpu().numpy())\n",
        "            except:\n",
        "                threshold_val = torch.quantile(att_scores, 0.6)  # 상위 40%\n",
        "\n",
        "            # Threshold 이상인 패치들 선택\n",
        "            mask = att_scores > threshold_val\n",
        "            selected_indices = torch.where(mask)[0]\n",
        "\n",
        "            # 선택된 패치 수 제한\n",
        "            if len(selected_indices) > max_selections:\n",
        "                # Top-K만 선택\n",
        "                _, top_indices = torch.topk(att_scores[selected_indices], max_selections)\n",
        "                selected_indices = selected_indices[top_indices]\n",
        "            elif len(selected_indices) < 2:  # 최소 2개는 선택\n",
        "                _, top_indices = torch.topk(att_scores, 2)\n",
        "                selected_indices = top_indices\n",
        "\n",
        "            # HR features 선택\n",
        "            selected_features = hr_features[b, selected_indices]  # [num_selected, feature_dim]\n",
        "\n",
        "            # 고정 크기로 패딩 (max_selections 크기)\n",
        "            if len(selected_indices) < max_selections:\n",
        "                padding_size = max_selections - len(selected_indices)\n",
        "                padding = torch.zeros(padding_size, feature_dim, device=hr_features.device)\n",
        "                selected_features = torch.cat([selected_features, padding], dim=0)\n",
        "\n",
        "            selected_hr_features.append(selected_features)\n",
        "\n",
        "            # Selection mask 생성 (시각화용)\n",
        "            binary_mask = torch.zeros_like(att_scores)\n",
        "            binary_mask[selected_indices] = 1\n",
        "            selection_masks.append(binary_mask)\n",
        "\n",
        "        selected_hr_features = torch.stack(selected_hr_features)  # [batch_size, max_selections, feature_dim]\n",
        "        selection_masks = torch.stack(selection_masks)           # [batch_size, 16]\n",
        "\n",
        "        return selected_hr_features, selection_masks\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 핵심! Hierarchical Self-Attention Module (FlexAttention 구현)\n",
        "# =============================================================================\n",
        "\n",
        "class HierarchicalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    FlexAttention 논문의 Hierarchical Self-Attention (Equations 3-7)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=384, num_heads=6, dropout=0.1):\n",
        "        super(HierarchicalSelfAttention, self).__init__()\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = feature_dim // num_heads\n",
        "\n",
        "        assert feature_dim % num_heads == 0\n",
        "\n",
        "        # Standard projections for hidden states (Eq. 3)\n",
        "        self.q_proj = nn.Linear(feature_dim, feature_dim)\n",
        "        self.k_proj = nn.Linear(feature_dim, feature_dim)\n",
        "        self.v_proj = nn.Linear(feature_dim, feature_dim)\n",
        "\n",
        "        # Separate projections for HR features (Eq. 4-5)\n",
        "        self.k_proj_hr = nn.Linear(feature_dim, feature_dim)  # W'_K\n",
        "        self.v_proj_hr = nn.Linear(feature_dim, feature_dim)  # W'_V\n",
        "\n",
        "        self.out_proj = nn.Linear(feature_dim, feature_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, hidden_states, hr_features):\n",
        "        \"\"\"\n",
        "        FlexAttention의 핵심 계산 (Equations 3-7)\n",
        "\n",
        "        Args:\n",
        "            hidden_states: [batch_size, seq_len, feature_dim] - LR + Global + CLS tokens\n",
        "            hr_features: [batch_size, num_hr_selected, feature_dim] - 선택된 HR features\n",
        "\n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, feature_dim] - 업데이트된 hidden states\n",
        "            attention_map: [batch_size, seq_len] - 다음 layer용 attention map\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        _, num_hr, _ = hr_features.shape\n",
        "\n",
        "        # Equation 3: Q = H * W_Q\n",
        "        Q = self.q_proj(hidden_states)\n",
        "        K_h = self.k_proj(hidden_states)\n",
        "        V_h = self.v_proj(hidden_states)\n",
        "\n",
        "        # Equation 4-5: K_all = Concat(H*W_K, f_SHR*W'_K)\n",
        "        K_hr = self.k_proj_hr(hr_features)  # W'_K (separate projection)\n",
        "        V_hr = self.v_proj_hr(hr_features)  # W'_V (separate projection)\n",
        "\n",
        "        # Concatenate keys and values\n",
        "        K_all = torch.cat([K_h, K_hr], dim=1)  # [batch_size, seq_len + num_hr, feature_dim]\n",
        "        V_all = torch.cat([V_h, V_hr], dim=1)  # [batch_size, seq_len + num_hr, feature_dim]\n",
        "\n",
        "        # Multi-head attention으로 reshape\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K_all = K_all.view(batch_size, seq_len + num_hr, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V_all = V_all.view(batch_size, seq_len + num_hr, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Equation 6: Hierarchical Self-attention 계산\n",
        "        scores = torch.matmul(Q, K_all.transpose(-2, -1)) / self.scale\n",
        "        attention_weights = F.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len + num_hr]\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Attention 적용\n",
        "        attended = torch.matmul(attention_weights, V_all)\n",
        "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.feature_dim)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.out_proj(attended)\n",
        "\n",
        "        # Equation 7: 다음 layer용 attention map 추출\n",
        "        # CLS token의 attention을 LR tokens에 대해서만 추출\n",
        "        cls_attention = attention_weights[:, :, -1, :seq_len-1]  # [batch_size, num_heads, seq_len-1]\n",
        "        attention_map = cls_attention.mean(dim=1)  # [batch_size, seq_len-1] - head들 평균\n",
        "\n",
        "        return output, attention_map\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 메인 FlexAttention MIL 모델\n",
        "# =============================================================================\n",
        "\n",
        "class FlexAttentionPatientMIL(nn.Module):\n",
        "    \"\"\"\n",
        "    Patient-Level FlexAttention MIL 모델\n",
        "\n",
        "    구조:\n",
        "    1. 환자별 20개 메가패치 → 각각 16개 패치 → 3-stream\n",
        "    2. LR + Global tokens → Self-Attention layers\n",
        "    3. LR attention → HR selection → FlexAttention layers\n",
        "    4. CLS token → Patient-level classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=384, num_classes=2, num_heads=6,\n",
        "                 num_sa_layers=1, num_fa_layers=2, dropout=0.1):\n",
        "        super(FlexAttentionPatientMIL, self).__init__()\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "        self.num_sa_layers = num_sa_layers\n",
        "        self.num_fa_layers = num_fa_layers\n",
        "\n",
        "        # Feature extractors\n",
        "        self.lr_extractor = ResNetFeatureExtractor(feature_dim=feature_dim)     # 64x64용\n",
        "        self.global_extractor = ResNetFeatureExtractor(feature_dim=feature_dim) # 64x64용\n",
        "        self.hr_extractor = ResNetFeatureExtractor(feature_dim=feature_dim)     # 256x256용\n",
        "\n",
        "        # CLS token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, feature_dim))\n",
        "\n",
        "        # Positional encoding (최대 320+20+1 = 341 tokens)\n",
        "        max_tokens = 400  # 충분한 여유\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, max_tokens, feature_dim))\n",
        "\n",
        "        # Standard Self-Attention layers (LR + Global + CLS)\n",
        "        self.sa_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=feature_dim * 4,\n",
        "                dropout=dropout,\n",
        "                batch_first=True\n",
        "            ) for _ in range(num_sa_layers)\n",
        "        ])\n",
        "\n",
        "        # FlexAttention layers\n",
        "        self.hr_selectors = nn.ModuleList([\n",
        "            HRFeatureSelector() for _ in range(num_fa_layers)\n",
        "        ])\n",
        "\n",
        "        self.hierarchical_attentions = nn.ModuleList([\n",
        "            HierarchicalSelfAttention(feature_dim, num_heads, dropout)\n",
        "            for _ in range(num_fa_layers)\n",
        "        ])\n",
        "\n",
        "        self.fa_ffns = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(feature_dim, feature_dim * 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(feature_dim * 4, feature_dim),\n",
        "                nn.Dropout(dropout)\n",
        "            ) for _ in range(num_fa_layers)\n",
        "        ])\n",
        "\n",
        "        self.fa_layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(feature_dim) for _ in range(num_fa_layers)\n",
        "        ])\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, lr_features, global_features, hr_features):\n",
        "        \"\"\"\n",
        "        FlexAttention MIL Forward Pass\n",
        "\n",
        "        Args:\n",
        "            lr_features: [batch_size, total_lr_patches, feature_dim] - 모든 LR features\n",
        "            global_features: [batch_size, num_megapatches, feature_dim] - Global features\n",
        "            hr_features: [batch_size, total_hr_patches, feature_dim] - 모든 HR features\n",
        "\n",
        "        Returns:\n",
        "            logits: [batch_size, num_classes] - Patient-level predictions\n",
        "            attention_maps: List of attention maps for visualization\n",
        "        \"\"\"\n",
        "        batch_size = lr_features.shape[0]\n",
        "\n",
        "        # Step 1: Token sequence 구성 (LR + Global + CLS)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "\n",
        "        # LR과 Global features 결합\n",
        "        # 메모리 효율성을 위해 일부만 사용 (큰 경우)\n",
        "        max_lr_tokens = min(lr_features.shape[1], 256)  # 최대 256개 LR tokens\n",
        "        max_global_tokens = min(global_features.shape[1], 20)  # 최대 20개 Global tokens\n",
        "\n",
        "        lr_subset = lr_features[:, :max_lr_tokens]\n",
        "        global_subset = global_features[:, :max_global_tokens]\n",
        "\n",
        "        # Initial token sequence: [LR tokens + Global tokens + CLS]\n",
        "        hidden_states = torch.cat([lr_subset, global_subset, cls_tokens], dim=1)\n",
        "\n",
        "        # Positional encoding 추가\n",
        "        seq_len = hidden_states.shape[1]\n",
        "        if seq_len <= self.pos_encoding.shape[1]:\n",
        "            hidden_states = hidden_states + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        attention_maps = []\n",
        "\n",
        "        # Step 2: Standard Self-Attention layers (Algorithm 1, lines 8-12)\n",
        "        for i in range(self.num_sa_layers):\n",
        "            hidden_states = self.sa_layers[i](hidden_states)\n",
        "\n",
        "        # Step 3: FlexAttention layers (Algorithm 1, lines 14-19)\n",
        "        for i in range(self.num_fa_layers):\n",
        "            # Step 3a: LR attention으로 HR selection (Algorithm 1, line 15)\n",
        "            if i == 0:\n",
        "                # 첫 번째 layer: uniform attention\n",
        "                num_lr_tokens = lr_subset.shape[1]\n",
        "                lr_attention_map = torch.ones(batch_size, num_lr_tokens, device=lr_features.device)\n",
        "                lr_attention_map = lr_attention_map / lr_attention_map.sum(dim=1, keepdim=True)\n",
        "            else:\n",
        "                # 이전 layer의 attention 사용\n",
        "                lr_attention_map = attention_maps[-1][:, :lr_subset.shape[1]]  # LR 부분만\n",
        "\n",
        "            # HR features를 LR과 동일한 크기로 맞춤 (패치 단위 대응)\n",
        "            hr_subset_size = min(hr_features.shape[1], lr_subset.shape[1])\n",
        "            hr_subset = hr_features[:, :hr_subset_size]\n",
        "            lr_attention_subset = lr_attention_map[:, :hr_subset_size]\n",
        "\n",
        "            # Step 3b: 중요한 HR features 선택\n",
        "            selected_hr_features, selection_mask = self.hr_selectors[i](\n",
        "                lr_attention_subset, hr_subset\n",
        "            )\n",
        "\n",
        "            # Step 3c: Hierarchical Self-Attention (Algorithm 1, line 16)\n",
        "            attended_output, new_attention_map = self.hierarchical_attentions[i](\n",
        "                hidden_states, selected_hr_features\n",
        "            )\n",
        "\n",
        "            # Step 3d: Skip connection (Algorithm 1, line 17)\n",
        "            hidden_states = hidden_states + attended_output\n",
        "\n",
        "            # Step 3e: Layer normalization\n",
        "            hidden_states = self.fa_layer_norms[i](hidden_states)\n",
        "\n",
        "            # Step 3f: FFN + skip connection (Algorithm 1, line 18)\n",
        "            ffn_output = self.fa_ffns[i](hidden_states)\n",
        "            hidden_states = hidden_states + ffn_output\n",
        "\n",
        "            attention_maps.append(new_attention_map)\n",
        "\n",
        "        # Step 4: Patient-level classification (Algorithm 1, line 20)\n",
        "        cls_output = hidden_states[:, -1]  # CLS token\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        return logits, attention_maps\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Dataset (환자별 데이터 처리)\n",
        "# =============================================================================\n",
        "\n",
        "class FlexAttentionBladderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FlexAttention용 환자별 Dataset\n",
        "    각 환자의 메가패치들을 처리해서 3-stream features 생성\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patient_data, target_type='t_label',\n",
        "                 max_megapatches_per_patient=20, cache_dir=None):\n",
        "        self.patient_data = patient_data\n",
        "        self.patient_ids = list(patient_data.keys())\n",
        "        self.target_type = target_type\n",
        "        self.max_megapatches_per_patient = max_megapatches_per_patient\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "        if cache_dir:\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        # Transforms\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_id = self.patient_ids[idx]\n",
        "        patient_info = self.patient_data[patient_id]\n",
        "\n",
        "        # Label 가져오기\n",
        "        label = patient_info.get(self.target_type, 0)\n",
        "        if label is None:\n",
        "            label = 0\n",
        "\n",
        "        # 이 환자의 모든 메가패치 처리\n",
        "        all_lr_features = []\n",
        "        all_global_features = []\n",
        "        all_hr_features = []\n",
        "\n",
        "        # 메가패치 수 제한 (메모리 효율성)\n",
        "        megapatch_paths = patient_info['images'][:self.max_megapatches_per_patient]\n",
        "\n",
        "        for megapatch_path in megapatch_paths:\n",
        "            try:\n",
        "                # 캐싱 확인\n",
        "                cache_key = None\n",
        "                if self.cache_dir:\n",
        "                    cache_key = hashlib.md5(megapatch_path.encode()).hexdigest()\n",
        "                    cache_path = os.path.join(self.cache_dir, f\"{cache_key}.pkl\")\n",
        "\n",
        "                    if os.path.exists(cache_path):\n",
        "                        with open(cache_path, 'rb') as f:\n",
        "                            processed = pickle.load(f)\n",
        "                    else:\n",
        "                        # 메가패치 처리: 1024x1024 → 16개 패치 → 3-stream\n",
        "                        processed = process_megapatch_complete(megapatch_path)\n",
        "\n",
        "                        # 캐싱 저장\n",
        "                        with open(cache_path, 'wb') as f:\n",
        "                            pickle.dump(processed, f)\n",
        "                else:\n",
        "                    # 캐싱 없이 처리\n",
        "                    processed = process_megapatch_complete(megapatch_path)\n",
        "\n",
        "                # 각 stream별로 tensor 변환\n",
        "                # LR patches: 16개 × 64x64\n",
        "                lr_tensors = []\n",
        "                for lr_patch in processed['lr_patches']:\n",
        "                    lr_pil = Image.fromarray(lr_patch)\n",
        "                    lr_tensor = self.transform(lr_pil)\n",
        "                    lr_tensors.append(lr_tensor)\n",
        "\n",
        "                # Global tokens: 16개 × 64x64 (모두 동일하므로 1개만 사용)\n",
        "                global_pil = Image.fromarray(processed['global_tokens'][0])  # 첫 번째 (모두 동일)\n",
        "                global_tensor = self.transform(global_pil)\n",
        "\n",
        "                # HR patches: 16개 × 256x256\n",
        "                hr_tensors = []\n",
        "                for hr_patch in processed['hr_patches']:\n",
        "                    hr_pil = Image.fromarray(hr_patch)\n",
        "                    hr_tensor = self.transform(hr_pil)\n",
        "                    hr_tensors.append(hr_tensor)\n",
        "\n",
        "                # 리스트에 추가\n",
        "                all_lr_features.extend(lr_tensors)    # 메가패치별 16개씩 누적\n",
        "                all_global_features.append(global_tensor)  # 메가패치별 1개씩\n",
        "                all_hr_features.extend(hr_tensors)    # 메가패치별 16개씩 누적\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {megapatch_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Dummy features if empty\n",
        "        if not all_lr_features:\n",
        "            dummy_lr = torch.zeros(3, 64, 64)\n",
        "            dummy_global = torch.zeros(3, 64, 64)\n",
        "            dummy_hr = torch.zeros(3, 256, 256)\n",
        "\n",
        "            all_lr_features = [dummy_lr] * 16\n",
        "            all_global_features = [dummy_global]\n",
        "            all_hr_features = [dummy_hr] * 16\n",
        "\n",
        "        # Tensor로 변환\n",
        "        lr_tensor = torch.stack(all_lr_features)      # [total_lr_patches, 3, 64, 64]\n",
        "        global_tensor = torch.stack(all_global_features)  # [num_megapatches, 3, 64, 64]\n",
        "        hr_tensor = torch.stack(all_hr_features)      # [total_hr_patches, 3, 256, 256]\n",
        "\n",
        "        return {\n",
        "            'patient_id': patient_id,\n",
        "            'lr_patches': lr_tensor,\n",
        "            'global_patches': global_tensor,\n",
        "            'hr_patches': hr_tensor,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# =============================================================================\n",
        "# 8. Training Function\n",
        "# =============================================================================\n",
        "\n",
        "def train_flexattention_model(patient_data, target_type='t_label', num_folds=3, num_epochs=15,\n",
        "                            batch_size=2, learning_rate=3e-4, device=device, save_dir='./checkpoints'):\n",
        "    \"\"\"FlexAttention MIL 모델 훈련\"\"\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    cache_dir = os.path.join(save_dir, 'cache')\n",
        "\n",
        "    # 데이터 준비\n",
        "    patient_ids = list(patient_data.keys())\n",
        "    patient_labels = [patient_data[pid].get(target_type, 0) for pid in patient_ids]\n",
        "    patient_labels = [0 if label is None else label for label in patient_labels]\n",
        "\n",
        "    # Stratified K-Fold\n",
        "    kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    results = {\n",
        "        'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(patient_ids, patient_labels)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Fold {fold+1}/{num_folds}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # 데이터 분할\n",
        "        train_patients = {patient_ids[i]: patient_data[patient_ids[i]] for i in train_idx}\n",
        "        test_patients = {patient_ids[i]: patient_data[patient_ids[i]] for i in test_idx}\n",
        "\n",
        "        # Dataset 생성\n",
        "        train_dataset = FlexAttentionBladderDataset(\n",
        "            train_patients, target_type=target_type, cache_dir=cache_dir\n",
        "        )\n",
        "        test_dataset = FlexAttentionBladderDataset(\n",
        "            test_patients, target_type=target_type, cache_dir=cache_dir\n",
        "        )\n",
        "\n",
        "        # DataLoader 생성\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=2, pin_memory=True, persistent_workers=True\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=2, pin_memory=True, persistent_workers=True\n",
        "        )\n",
        "\n",
        "        # 모델 초기화\n",
        "        model = FlexAttentionPatientMIL(\n",
        "            feature_dim=384, num_classes=2, num_heads=6,\n",
        "            num_sa_layers=1, num_fa_layers=2, dropout=0.1\n",
        "        )\n",
        "\n",
        "        # DataParallel for RTX 6000 Ada x2\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
        "            model = nn.DataParallel(model)\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Optimizer & Scheduler\n",
        "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "        total_steps = len(train_loader) * num_epochs\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps)\n",
        "\n",
        "        # Loss & Scaler\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scaler = GradScaler()\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "                lr_patches = batch['lr_patches'].to(device)       # [batch_size, num_lr, 3, 64, 64]\n",
        "                global_patches = batch['global_patches'].to(device)  # [batch_size, num_global, 3, 64, 64]\n",
        "                hr_patches = batch['hr_patches'].to(device)       # [batch_size, num_hr, 3, 256, 256]\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with autocast():\n",
        "                    # Feature extraction\n",
        "                    batch_size, num_lr, C, H_lr, W_lr = lr_patches.shape\n",
        "                    _, num_global, _, H_global, W_global = global_patches.shape\n",
        "                    _, num_hr, _, H_hr, W_hr = hr_patches.shape\n",
        "\n",
        "                    # Flatten for feature extraction\n",
        "                    lr_flat = lr_patches.view(-1, C, H_lr, W_lr)\n",
        "                    global_flat = global_patches.view(-1, C, H_global, W_global)\n",
        "                    hr_flat = hr_patches.view(-1, C, H_hr, W_hr)\n",
        "\n",
        "                    # Feature extractors\n",
        "                    if hasattr(model, 'module'):\n",
        "                        lr_extractor = model.module.lr_extractor\n",
        "                        global_extractor = model.module.global_extractor\n",
        "                        hr_extractor = model.module.hr_extractor\n",
        "                    else:\n",
        "                        lr_extractor = model.lr_extractor\n",
        "                        global_extractor = model.global_extractor\n",
        "                        hr_extractor = model.hr_extractor\n",
        "\n",
        "                    # Extract features\n",
        "                    lr_features = lr_extractor(lr_flat)      # [batch_size * num_lr, feature_dim]\n",
        "                    global_features = global_extractor(global_flat)  # [batch_size * num_global, feature_dim]\n",
        "                    hr_features = hr_extractor(hr_flat)      # [batch_size * num_hr, feature_dim]\n",
        "\n",
        "                    # Reshape back\n",
        "                    lr_features = lr_features.view(batch_size, num_lr, -1)\n",
        "                    global_features = global_features.view(batch_size, num_global, -1)\n",
        "                    hr_features = hr_features.view(batch_size, num_hr, -1)\n",
        "\n",
        "                    # Forward through FlexAttention MIL\n",
        "                    if hasattr(model, 'module'):\n",
        "                        logits, attention_maps = model.module(lr_features, global_features, hr_features)\n",
        "                    else:\n",
        "                        logits, attention_maps = model(lr_features, global_features, hr_features)\n",
        "\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            avg_loss = total_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # 체크포인트 저장\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(save_dir, f'fold_{fold+1}_epoch_{epoch+1}.pt')\n",
        "                model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model_state,\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': avg_loss,\n",
        "                }, checkpoint_path)\n",
        "                print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "                lr_patches = batch['lr_patches'].to(device)\n",
        "                global_patches = batch['global_patches'].to(device)\n",
        "                hr_patches = batch['hr_patches'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                # Feature extraction (동일한 과정)\n",
        "                batch_size, num_lr, C, H_lr, W_lr = lr_patches.shape\n",
        "                _, num_global, _, H_global, W_global = global_patches.shape\n",
        "                _, num_hr, _, H_hr, W_hr = hr_patches.shape\n",
        "\n",
        "                lr_flat = lr_patches.view(-1, C, H_lr, W_lr)\n",
        "                global_flat = global_patches.view(-1, C, H_global, W_global)\n",
        "                hr_flat = hr_patches.view(-1, C, H_hr, W_hr)\n",
        "\n",
        "                if hasattr(model, 'module'):\n",
        "                    lr_features = model.module.lr_extractor(lr_flat)\n",
        "                    global_features = model.module.global_extractor(global_flat)\n",
        "                    hr_features = model.module.hr_extractor(hr_flat)\n",
        "                    logits, _ = model.module(\n",
        "                        lr_features.view(batch_size, num_lr, -1),\n",
        "                        global_features.view(batch_size, num_global, -1),\n",
        "                        hr_features.view(batch_size, num_hr, -1)\n",
        "                    )\n",
        "                else:\n",
        "                    lr_features = model.lr_extractor(lr_flat)\n",
        "                    global_features = model.global_extractor(global_flat)\n",
        "                    hr_features = model.hr_extractor(hr_flat)\n",
        "                    logits, _ = model(\n",
        "                        lr_features.view(batch_size, num_lr, -1),\n",
        "                        global_features.view(batch_size, num_global, -1),\n",
        "                        hr_features.view(batch_size, num_hr, -1)\n",
        "                    )\n",
        "\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().tolist())\n",
        "                all_labels.extend(labels.cpu().tolist())\n",
        "                all_probs.extend(probs[:, 1].cpu().tolist())\n",
        "\n",
        "        # 메트릭 계산\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(all_labels, all_probs)\n",
        "        except:\n",
        "            auc = 0.0\n",
        "\n",
        "        print(f\"\\nFold {fold+1} Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1: {f1:.4f}\")\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "        results['accuracy'].append(accuracy)\n",
        "        results['precision'].append(precision)\n",
        "        results['recall'].append(recall)\n",
        "        results['f1'].append(f1)\n",
        "        results['auc'].append(auc)\n",
        "\n",
        "        # 최종 모델 저장\n",
        "        final_model_path = os.path.join(save_dir, f'final_model_fold_{fold+1}.pt')\n",
        "        model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
        "        torch.save(model_state, final_model_path)\n",
        "\n",
        "        # 메모리 정리\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 최종 결과\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Average Accuracy: {np.mean(results['accuracy']):.4f} ± {np.std(results['accuracy']):.4f}\")\n",
        "    print(f\"Average Precision: {np.mean(results['precision']):.4f} ± {np.std(results['precision']):.4f}\")\n",
        "    print(f\"Average Recall: {np.mean(results['recall']):.4f} ± {np.std(results['recall']):.4f}\")\n",
        "    print(f\"Average F1: {np.mean(results['f1']):.4f} ± {np.std(results['f1']):.4f}\")\n",
        "    print(f\"Average AUC: {np.mean(results['auc']):.4f} ± {np.std(results['auc']):.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# 9. Main Execution\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 경로 설정\n",
        "    zip_path = \"/home/ubuntu/ExternalUSB_Bladder_240710.zip\"\n",
        "    base_dir = \"/home/ubuntu/ExternalUSB_Bladder_240710\"\n",
        "    excel_path = \"/home/ubuntu/MIL_TURB_240918_Modified_LambdaLabs.xlsx\"\n",
        "\n",
        "    print(\"Loading and matching data...\")\n",
        "    # [기존 데이터 로딩 코드 사용]\n",
        "\n",
        "    # T-stage 분류 훈련\n",
        "    print(\"\\nTraining FlexAttention MIL for T-stage classification...\")\n",
        "    t_results = train_flexattention_model(\n",
        "        patient_data=patient_data,  # 로딩된 데이터\n",
        "        target_type='t_label',\n",
        "        num_folds=3,\n",
        "        num_epochs=15,\n",
        "        batch_size=2,\n",
        "        learning_rate=3e-4,\n",
        "        device=device,\n",
        "        save_dir='./checkpoints_t_stage'\n",
        "    )\n",
        "\n",
        "    # Recurrence 예측 훈련\n",
        "    print(\"\\nTraining FlexAttention MIL for recurrence prediction...\")\n",
        "    recur_results = train_flexattention_model(\n",
        "        patient_data=patient_data,\n",
        "        target_type='recur_label',\n",
        "        num_folds=3,\n",
        "        num_epochs=15,\n",
        "        batch_size=2,\n",
        "        learning_rate=3e-4,\n",
        "        device=device,\n",
        "        save_dir='./checkpoints_recurrence'\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining completed!\")"
      ]
    }
  ]
}